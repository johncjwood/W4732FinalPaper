% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{101} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2024}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Beyond Lip Reading - Inferring Voice Qualities from Visual Data}

\author{John Wood, 
Columbia University
{\tt\small john.wood@columbia.edu}
\and Jacob Wahbeh,
Columbia University
{\tt\small jacob.wahbeh@columbia.edu}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
   Whereas there have been many successful and well-studied methodologies, symbolic representations, and processes for inferring words from video / visual-only data, the reproductions of voice have been lacking in reproducing qualities of voice such as pitch, intensity, shimmer, jitter, and harmonics. We have worked towards building a corpus and model to attempt to recover these qualities from the visual data.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

There have been no shortages of papers and processes for distilling spoken words from visual-only data sources, i.e. performing lip reading. Fenghour \etal 2021 \cite{Fenghour2021} does a thorough job reviewing the corpora, pre-processing techniques, neural network architectures, and results of these processes. Ultimately, the target goal is to produce understandable words / sentences for human consumption. However, there are many other complexities to human vocal communication which these methods are not trained to reproduce. As a simple example, a rising pitch can suggest if the speaker is posing the statement as a question or topic of discussion versus a fact. After all, as we speak, the qualities of our voice modulate due to changes in emotional state and as we purposefully modulate our voice to convey meaning and context such as excitement, boredom, sarcasm, amongst other information. 

%-------------------------------------------------------------------------
\subsection{Motivating Findings}

The work that motivated our initial interest in this particular problem is our observations of attempts to  recreate voice from video data. For instance, while RobustL2S by Sahipjohn \etal \cite{RobustL2S} does a remarkable job at inferring the words of the speakers with lip reading alone, the pitch and intensity of speaking in the resulting audio recreations can vary from the original data. For independent examination of their results, Sahipjohn \etal have provided a stellar website presenting their work: \href{https://neha-sherin.github.io/RobustL2S/}{https://neha-sherin.github.io/RobustL2S/}.

%------------------------------------------------------------------------
\section{Related Work}
\label{sec:related}

There is a lot of related work across multiple disciplines including prior work involving lip reading automation, emotion detection automation, and voice to image generation, and video to voice generation. 

%-------------------------------------------------------------------------

\subsection{Auditory and Visual Emotion Detection Automation}

One of the elements that forms the basis of our hypothesis that vocal traits can be recovered from visual information is the twofold existence of evidence that emotion can be recovered from visual information and that emotion can / does result in changes in vocal qualities versus the baseline. The relevance of both audio and video information to predicting emotion is discussed thoroughly in Zhicheng \etal 2023 \cite{EmotVid2023}, where independently, video and audio are shown to be predictive of emotion. Together, they are shown to be even more efficacious. Reinforcing the vocal quality side of the equation, Saini \etal 2023\cite{EmotPitch2023} discusses the relationship in more detail about how pitch and intensity, amongst other vocal cues, can be fundamental to recovering emotion.

\subsection{Voice to Image Generation}

Speech2Face\cite{Speech2face} by Oh \etal suggests a useful predictive correlation between one's appearance and one's vocal qualities. On a philosophical note, we are hesitant to proliferate models and methodologies which (potentially / likely) use individuals' protected characteristics to create inferences. It is in this spirit that, while our expectation for our resultant work to find some success in predicting the median speaker qualities based on image / video alone, to judge the efficacy of the model on its relative predictions of rising and falling vocal qualities, not on absolute resulting values.

\subsection{Lip Reading Automation}

At the time of writing, we feel confident that lip reading automation is a solved problem. While there are many approaches and methodologies (mentioned earlier in Fenghour \etal 2021 \cite{Fenghour2021}) , we find the viseme to be the most compelling fundamental building block (either explicitly or implicitly extracted) from which models can be built. Explored more thoroughly in Fenghour \etal 2023\cite{Fenghour2023}, visemes are the clusters of facial movements which occur when making a particular vocalization / set of vocalizations. For instance, as noted in Jachimski \etal 2017 \cite{Elephant2017} , "elephant juice" and "I love you" involve the same fundamental lip movements, in the same order, to produce. Once the building blocks are gathered, then it is then incumbent on using an LLM or equivalent to weight the relative probabilities of the various potential constructions using the visemes to ultimately provide the most likely words / sentences spoken. 

\subsection{Video to Voice Generation}


\subsection{Existing Video Corpora}

Some words here

%------------------------------------------------------------------------
\section{Methodology}
\label{sec:methodology}

Our methodology for performing this research was extensive due to the nature of the problem - specifically, we needed a corpus that would represent both a speaker's baseline audio characteristics (pitch / intensity / harmonics / etc.) as well as their audio characteristics when they deviate from that baseline. 

To achieve this, we developed our own corpus based on long-form conversations conducted in a semi-consistent environment. The long-form nature of the source datasets would facilitate 1) having enough data for an individual speaker to infer a baseline and 2) provide ample time (and stimulation) for the conversation to inspire a diversity of emotions that would cause the speaker to deviate from their baseline speaking qualities. We then collected the necessary metadata from the models using standard speech analysis tooling and libraries to create the speaker diarization and ultimately the audio metadata. 

We then purposefully avoided a common processing step in lip-reading models (for instance, used by Sahipjohn \etal \cite{RobustL2S} )  which is to use a Lip Encoder, i.e. a model which will remove excess visual data and zero in on the lips itself. We purposefully removed this to allow for the capture of other information that may be predictive of these other audio qualities. For instance, the distance between a speaker and the microphone may be conducive to predict intensity of speech. Alternatively, the upward movement of the eyebrows relative to the eyes may be predictive of a raised pitch, as the entirety of the facial expression may express surprise or the onset of a questioning mood.   

%-------------------------------------------------------------------------
\subsection{Data Collection}

As our "dynamic" data set, we chose the "Joe Rogan Experience Podcast", available as MP4 audio / video files on archive.org. We chose this due to the numeracy of the episodes, the diversity of speakers, the length of each episode (with many eclipsing 2 hours), and the dynamism of the host, Joe Rogan. We pulled select episodes from 200 to 600, limited to episodes with 1 guest which did not span multiple episodes, and also removed conversations where the speaker was not visible speaking, as was the case the the John McAfee episode.

\subsection{Data Processing}

Once we had the mp4 files, we extracted the video and audio into separate files. For the audio files, we performed speaker diarization (i.e. split the audio file into segments by the speaker) using the pyannote.audio library\cite{Bredin23}. We then fed these segmented audio files into a Pratt\cite{Pratt}-based library called parselmouth\cite{Parselmouth}.  Parselmouth is a convenient python-based tool we used to extract the voice qualities we were looking for, in 0.1 second intervals, which would become the basis for the data points our model would attempt to predict. We also accumulated this data to create speaker median values, which we would predict in tandem with the temporally-associated values. Out hope was that even if the model fails to correctly predict the median values, we could see predictions that were correctly above or below the median when relevant. 

\subsection{Model Construction}

Some words here

%------------------------------------------------------------------------
\section{Experimental Results}
\label{sec:results}

This and that

%-------------------------------------------------------------------------
\subsection{Experiment Topic 1}

Some words here

%------------------------------------------------------------------------
\section{Discussion}
\label{sec:discussion}

Here's where we discuss further

%-------------------------------------------------------------------------
\subsection{Discussion Topic}

Some words here

%------------------------------------------------------------------------
\section{Conclusion}
\label{sec:conclusion}

Some words here

%-------------------------------------------------------------------------
\subsection{Concluding point 1}

Some words here



%-------------------------------------------------------------------------


\begin{figure}[t]
  \centering
  \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   %\includegraphics[width=0.8\linewidth]{egfigure.eps}

   \caption{Example of caption.
   It is set in Roman so that mathematics (always set in Roman: $B \sin A = A \sin B$) may be included without an ugly clash.}
   \label{fig:onecol}
\end{figure}


%-------------------------------------------------------------------------
\subsection{Illustrations, graphs, and photographs}

All graphics should be centered.
In \LaTeX, avoid using the \texttt{center} environment for this purpose, as this adds potentially unwanted whitespace.
Instead use
{\small\begin{verbatim}
  \centering
\end{verbatim}}
at the beginning of your figure.
Please ensure that any point you wish to make is resolvable in a printed copy of the paper.
Resize fonts in figures to match the font in the body text, and choose line widths that render effectively in print.
Readers (and reviewers), even of an electronic copy, may choose to print your paper in order to read it.
You cannot insist that they do otherwise, and therefore must not assume that they can zoom in to see tiny details on a graphic.

When placing figures in \LaTeX, it's almost always best to use \verb+\includegraphics+, and to specify the figure width as a multiple of the line width as in the example below
{\small\begin{verbatim}
   \usepackage{graphicx} ...
   \includegraphics[width=0.8\linewidth]
                   {myfile.pdf}
\end{verbatim}
}


%-------------------------------------------------------------------------


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
